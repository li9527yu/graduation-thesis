\chapter{第三章xx}
现有的
\section{引言}
在第二章的工作中，我们定义了方面级情感相关性，基于相关性定义构建了情感相关性数据集。同时为验证提出的相关性的有效，我们提出了一种相关性感知的多模态方面级情感分类方法，通过相关性插件将相关性信息以图像权重比例的方式注入到模型之中。实验结果在多个模型和多个数据集得到了一致的提升表明了加入我们相关性的有效性。然而，现有方法在建模模态图文的情感线索时，主要通过全局的视觉特征或单一的文本语义 embedding 来代表模态情绪，但真实场景中的情绪线索往往分布在更细粒度的层面。例如，用户在文本中可能使用比喻、隐含情绪或未显式提及评价对象的表达方式；图像中的颜色、表情、场景构成等细节也可能蕴含间接的情绪信息。如果仅使用粗粒度的模态表示，模型难以准确捕捉这些细微却关键的情绪因素，从而导致情感识别偏差。另一方面。在上一章的工作主要通过计算图文是否相关的粗粒度权重系数来控制图像特征的使用，这种方式缺乏对细粒度情感相关性的深度挖掘。在复杂的实际场景中，图像中可能包含与文本情感无关的干扰背景，若仅进行粗粒度的权重调节，模型往往难以精确调控不同线索对最终情感决策的贡献，容易受到噪声干扰。
面对上述问题，本章认为可以引入多模态大语言模型(MLLM)通过显式知识增强的方式提升特征质量。基于该思路，本章提出了一种基于 MLLM 知识增强与细粒度相关性控制的多模态方面级情感分类方法。该方法首先利用 MLLM 抽取图像与文本中的情感线索描述，使得隐含的模态情绪信息能够以自然语言形式显式表达，也通过文本化的方式弥合模态间的表征差异；其次，基于情感相关性构建差异化的输入指令，引导模型在关注有用线索的同时主动忽略无关信息的干扰；最后，设计了一种两路分支的后期融合机制，实现对文本与图像分支占比的灵活调节。
总的来说，本章工作的主要贡献如下：
- 显式情感知识增强机制： 本章提出利用 MLLM 强大的推理能力，显式地从原始模态中抽取出深层的情感线索描述。通过将隐式的视觉特征转化为显式的文本语义线索，显著提升了情感特征的判别性与质量。
- 基于相关性的精细化线索控制： 本章设计了一套基于情感相关性的指令构建策略。通过在输入端引入不同类型的引导指令，使模型能够根据线索的贡献度进行动态调整，有效解决了由于线索粒度过粗而导致的抗噪能力弱的问题。
- 可控的多路融合架构： 本章提出了一种两路分支的后期融合方式。根据情感相关性标签调节两条分支的贡献比例，从而有效抑制无关或冲突模态的影响，突出真正与 aspect 相关的模态情绪信号。

\section{基于图文情感线索引导的多模态方面情感分类方法}
% 涉及到的符号表示，先整理下：
% - 图像
% - 文本
% -方面
% - 情感标签
% - 相关性标签
% -  文本情感线索
% - 图像情感线索
% - 文本分支输入
% - 图像分支输入
% - q-former图像表示
% - 文本分支输入表示
% - 图像分支输入表示
% - 两路分支各自和q-former图像表示拼接的表示
% - 两路分支的融合表示

本章提出了一种基于相关性控制与情感线索引导的多模态方面级情感分类方法(RC-ECG)，本章设计了一个包含离线数据增强与在线生成式分类的双阶段框架。具体来说，我们利用多模态大模型的强大能力来获得细粒度的方面级情感跨模态相关性，并从图像和文本中提取显式的、与方面相关的情感线索描述，为后续的分类任务提供参考信息。我们得到了包含情感线索描述和方面级情感跨模态相关性标签的扩充数据集。接着在相关性引导的情感线索训练阶段中，我们根据情感线索描述和相关性标签来动态构建文本分支与图像分支的提示输入，筛选出与目标方面相关的情感线索。最后，我们设计了一种两路分支的后期融合机制，根据情感相关性标签调节两条分支的贡献比例，从而有效抑制无关或冲突模态的影响，突出真正与目标方面相关的模态情绪信号。
% 
\subsection{方面级情感跨模态相关性与图文情感线索描述数据集构建}
为满足本章提出的基于相关性控制与情感线索引导的多模态方面情感分类方法，我们为每个样本添加方面级情感跨模态相关性标签与情感线索描述。因此我们首先需要基于原有的图文对来构建情感线索描述和方面级情感跨模态相关性标签。具体来说，我们分别使用 Qwen2.5-VL-7B 模型和 Gemini 2.5来进行数据集的扩充。整体的构建流程如图所示，接下来是数据构建的具体细节：

\textbf{方面级情感跨模态相关性标签生成}。
对于数据集中的样本$x$,为了精准描述其中图像 $x^i$与文本 $x^t$中方面$x^a$之间的情感跨模态相关性，我们选用在视觉理解与推理任务上表现优异的 Qwen2.5-VL-7B 模型作为方面级情感跨模态相关性的分类器。具体来说，我们基于上一章构建的 AECR-Twitter 数据集，对 Qwen2.5-VL-7B 进行指令微调，然后在Twitter-15和Twitter-17数据集进行推理，使其能够根据图文内容输出三分类标签 $r \in \{0, 1, 2\}$。这些标签将作为后续生成式框架中引导情感线索组合使用的核心控制信号，用于筛选出与目标方面相关的情感线索。
\begin{itemize}
    \item 无关 ($r=0$)：图像内容与文本或目标方面存在矛盾或无关联，视为噪声。
    \item 语义相关 ($r=1$)：图像在语义信息上与文本一致，但未包含明显的情感倾向。
    \item 情感相关 ($r=2$)：图像表达或加强了针对目标方面的情感表达。
\end{itemize}

\textbf{图文情感线索描述生成}。为了弥补小参数模型在情感理解能力上的不足，我们利用MLLM的强大的推理能力提取显式的情感线索描述作为一种知识蒸馏的方式。我们通过构建prompt调用MLLM分别从图像 $x^i$与文本 $x^t$中提取方面$x^a$相关的情感线索描述。具体来说，对于图像情感线索描述$c^i$，我们输入图片和目标方面词$x^a$，要求模型输出与$x^a$ 相关的情感线索描述。而对于文本情感线索描述$c^t$，我们输入文本 $x^t$ 和目标方面词$x^a$，要求模型输出与$x^a$相关的情感线索描述。具体的prompt如图所示。

\subsection{相关性引导的生成式框架}
在相关性引导的情感线索训练阶段中，我们根据上一节得到的情感线索描述和相关性标签来动态构建文本分支与图像分支的提示入喉输入到该生成式框架中。具体来说，该框架主要由相关性感知的提示构建、图文输入编码、以及两路分支融合三个模块组成。
\subsection{相关性感知的提示构建}
为了减轻无关情感线索的干扰，我们设计了一种方面级情感跨模态相关性感知的提示工程策略。我们构建了两个输入序列：文本分支提示, $X_{txt}$和图像分支提示 $X_{multi}$。其构建逻辑严格受控于方面级情感跨模态相关性标签 $r$：文本分支提示 ($X_{txt}$)，该分支专注于文本模态的情感线索描述。我们将输入文本 $T$、目标方面$x^a$ 以及提取的文本线索 $C_{txt}$ 按照标准模板进行拼接：
\begin{equation}
    X_{txt} = \text{``Aspect: } A \text{, Sentence: } T \text{, Clues: } C_{txt}''
\end{equation}
图像分支提示，该分支旨在融入图像模态的情感线索描述，但通过相关性标签 $r$ 进行控制是否引入。具体构建逻辑如下：
  \begin{equation}
  X_{multi} = 
  \begin{cases} 
  \mathcal{T}_{irrel}(T, A), & \text{if } r = 0 \\
  \mathcal{T}_{rel}(T, A, C_{img}), & \text{if } r \in \{1, 2\}
  \end{cases}
  \end{equation}
具体而言，当 $r=0$（无关）时，我们丢弃图像线索 $C_{img}$，并使用一条限制性指令$\mathcal{T}_{irrel}$，强制模型忽略视觉输入。当 $r \ge 1$（相关）时，我们整合 $C_{img}$ 并使用一条协同指令$\mathcal{T}_{rel}$，结合图像情感线索 $C_{img}$ 来引导模型关注与目标方面相关的情感线索。具体而言，$\mathcal{T}_{rel}$ 模板为：  
\begin{equation}
    \mathcal{T}_{rel}(T, A, C_{img}) = \text{``Aspect: } A \text{, Sentence: } T \text{, Clues: } C_{img}''
\end{equation}

\subsection{编码器输入}
在本小节中，模型的输入层旨在将图文模态特征统一映射至预训练语言模型的语义空间。我们分别对视觉特征和文本提示进行处理，构建包含视觉前缀的序列化嵌入。具体来说，对于图像特征的编码，我们参考前人的工作利用强大的MLLM来获得图像中与方面相关的图像特征，首先对于图像 $x^i$，我们使用冻结的图像编码器ViT-g/14来提取图像特征 $\mathbf{h}^{img}$，然后我们创建一组可学习的查询嵌入q，这些查询可以通过自注意力层与方面特征交互，也可以通过交叉注意力层与图像特征交互。正如InstructBLIP将图像特征和指令一起输入到Q-Former中，以使查询Z提取出指令所描述的任务信息更丰富的图像特征一样，我们也同时将图像特征 $\mathbf{h}^{img}$和方面$x^a$输入到Q-Former中，以获得与图像方面相关的细粒度特征。由于 Q-Former 已经过预先训练，可以通过查询提取包含语言信息的视觉表示，因此它可以有效地充当信息瓶颈，提供最有用的信息，同时过滤掉不相关的视觉信息。最后，我们得到最终的隐藏状态表示$h^f$。值得注意的是，文本分支与图像分支共享该投影模块的参数，这有助于模型在统一的视觉语义空间下进行推理。
对应的数学公式，vit和q-former部分
图像特征提取：
\begin{equation}
\mathbf{h}^{img} = \text{ViT-g/14}(\mathbf{x}^i)
\end{equation}
方面特征提取：
\begin{equation}
\mathbf{h}^{a} = \text{Flan-T5}(\mathbf{x}^a)
\end{equation}
Q-Former 编码：
\begin{equation}
\mathbf{h}^{f} = \text{Q-Former}(\mathbf{h}^{img}, \mathbf{h}^{a})
\end{equation}
接着基于前文构建的文本主导提示序列 $X_{txt}$ 与多模态主导提示序列 $X_{multi}$，我们首先通过冻结的 T5 词嵌入层 $\text{Embed}(\cdot)$ 将其转换为离散的文本嵌入。随后，将生成的软视觉前缀 $\mathbf{P}_{vis}$ 分别拼接至两个分支的文本嵌入序列前端，形成最终的编码器输入：
\begin{equation}
\begin{aligned}
\mathbf{E}_{txt} &= [\mathbf{P}_{vis}; \text{Embed}(X_{txt})] \\
\mathbf{E}_{multi} &= [\mathbf{P}_{vis}; \text{Embed}(X_{multi})]
\end{aligned}
\end{equation}
其中 $[\cdot ; \cdot]$ 表示在序列长度维度上的级联操作。通过这种前缀注入机制，双流分支在接收相同视觉信息的同时，受到不同指令与情感线索的引导，从而实现差异化的特征编码。
 

\subsection{解码器输出和双流融合}
构建好的输入嵌入 $\mathbf{E}_{txt}$ 与 $\mathbf{E}_{multi}$ 被并行输入至参数共享的 Flan-T5 生成式模型中。为了综合利用文本模态的稳定性与视觉模态的丰富性，我们在解码阶段采用逻辑层融合策略。在解码步骤 $t$，模型分别为两个分支计算当前时间步的隐状态，并映射至词表空间，得到预测分布：
\begin{equation}
\mathbf{z}_{txt}^{(t)} = \mathcal{F}_{\theta}(\mathbf{E}_{txt}, y_{<t}), \quad \mathbf{z}_{multi}^{(t)} = \mathcal{F}_{\theta}(\mathbf{E}_{multi}, y_{<t})
\end{equation}
其中 $\mathcal{F}_{\theta}$ 表示共享参数的模块，$y_{<t}$ 为 $t$ 时刻之前生成的历史 Token 序列。
同时我们通过引入一个可调节的平衡超参数 $\lambda \in [0, 1]$，对两个分支的 Logits 进行加权求和，得到最终的融合分布 $\mathbf{z}_{final}^{(t)}$：
\begin{equation}
\mathbf{z}_{final}^{(t)} = \lambda \cdot \mathbf{z}_{txt}^{(t)} + (1 - \lambda) \cdot \mathbf{z}_{multi}^{(t)}
\end{equation}
该融合机制允许模型根据任务需求灵活调整对特定模态的依赖。在生成阶段，模型基于融合后的 Logits 采用贪婪解码策略选择概率最大的 Token 作为输出：
\begin{equation}
\hat{y}_t = \arg\max(\mathbf{z}_{final}^{(t)})
\end{equation}

\subsection{损失函数}
模型的训练采用端到端的监督学习方式。我们的目标是最小化预测序列与真实情感标签序列 $Y = \{y_1, y_2, \dots, y_L\}$ 之间的差异。采用标准的交叉熵损失函数（Cross-Entropy Loss），基于融合后的 Logits 计算损失：

\begin{equation}
\mathcal{L} = -\frac{1}{L} \sum_{t=1}^{L} \log \left( \frac{\exp(\mathbf{z}_{final}^{(t)}[y_t])}{\sum_{v \in \mathcal{V}} \exp(\mathbf{z}_{final}^{(t)}[v])} \right)
\end{equation}

其中，$L$ 为目标序列长度，$\mathcal{V}$ 表示预训练模型的词表大小，$\mathbf{z}_{final}^{(t)}[y_t]$ 表示在时间步 $t$ 真实标签对应的 Logit 值。通过最小化该损失函数，模型能够联合优化视觉投影层参数与解码策略，从而实现对多模态方面级情感的精准分类。
% #### 3.2 相关性引导的生成式框架
% 本节详细介绍在线训练阶段的核心架构。该框架包含相关性感知提示构建、视觉前缀投影与双流编码、以及自适应逻辑层融合三个模块。

% 3.2.1 相关性感知的提示构建
% （此处保持之前的内容不变，重点描述 $X_{txt}$ 和 $X_{multi}$ 的构建逻辑，以及 $R$ 如何控制 $X_{multi}$ 的指令模板。）

% 3.2.2 视觉前缀投影与双流编码
% 为了在引入视觉信息的同时避免破坏预训练语言模型的语义空间，我们采用了参数高效的视觉前缀注入（Visual Prefix Injection）机制。

% 1. 方面感知的视觉特征提取 (Aspect-Aware Visual Feature Extraction)
% 不同于直接使用整张图像的全局特征，本章旨在提取与目标方面$x^a$ 密切相关的局部视觉特征。我们利用 InstructBLIP 预训练模型中的 Q-Former 结构来实现这一目标。
% 具体而言，我们将整张原始图像作为视觉输入，并将目标方面词（Aspect Term）作为文本条件输入 Q-Former。Q-Former 内部包含一组可学习的查询向量（Learned Queries）。通过交叉注意力机制（Cross-Attention），这些查询向量在方面词的语义引导下，动态地关注图像中与该方面最相关的区域（例如，当方面为“服务员”时，模型聚焦于人物区域；当方面为“环境”时，模型聚焦于背景区域）。
% 最终，Q-Former 输出的特征 $\mathbf{H}_v \in \mathbb{R}^{K \times d_v}$ 紧凑地编码了方面感知的视觉信息，而非冗余的全局图像信息。

% 2. 视觉前缀投影 (Visual Prefix Projection)
% 为了将提取的视觉特征 $\mathbf{H}_v$ 适配到 Flan-T5 的嵌入空间，我们设计了两个独立的线性投影层（Linear Projection Layers），分别记为 $\phi_{txt}$ 和 $\phi_{multi}$。
% \begin{equation}
% \mathbf{P}_{txt} = \phi_{txt}(\mathbf{H}_v), \quad \mathbf{P}_{multi} = \phi_{multi}(\mathbf{H}_v)
% \end{equation}
% 这两个投影层将同一组视觉特征映射为两个不同的软视觉提示（Soft Visual Prompts），分别用于引导文本流和多模态流的编码。这种解耦设计允许模型在不同的分支中关注视觉信息的不同侧面。

% 3. 双流并行编码
% 生成的软提示被拼接在离散文本提示的词嵌入之前：
% \begin{equation}
% \mathbf{E}_{txt} = [\mathbf{P}_{txt}; \text{Embed}(X_{txt})], \quad \mathbf{E}_{multi} = [\mathbf{P}_{multi}; \text{Embed}(X_{multi})]
% \end{equation}
% 随后，$\mathbf{E}_{txt}$ 和 $\mathbf{E}_{multi}$ 被并行输入共享参数的 Flan-T5 骨干网络中，得到各自的解码器输出状态。

% 3.2.3 自适应逻辑层融合
% （此处保持之前的内容不变，重点描述 $\lambda$ 加权求和 $\mathbf{z}_{final}$ 以及 Loss 计算。）


\section{实验设置与结果分析}

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
\begin{table}[htbp]
\centering
\caption{\label{second:mainresult}
    对比实验结果
    }
\begin{tabular}{l|llll}
\toprule
\multirow{2}{*}{\textbf{Methods}} & \multicolumn{2}{l}{\textbf{Twitter-15}} & \multicolumn{2}{l}{\textbf{Twitter-17}} \\
   & ACC                 & F1                 & ACC                 & F1                 \\ \hline
ITM                               & 78.36               & 74.09              & 72.93               & 71.86              \\
HIMT                              & 76.37               & 72.01              & 68.82               & 66.88              \\
DPFN                              & 76.53               & 72.46              & 71.12               & 69.38              \\
Aom                               & 77.9                & 73.84              & 73.52               & 72.38              \\
Qwen2-VL-7B                       & \textbf{79.36}               & 74.25              & 73.39               & 72.61              \\
Qwen2.5-VL-7B                     & 77.62               & 73.73              & 74.31               & 73.95              \\
\hline
Base Model         & 77.77               & 73.13              & 74.11               & 72.74              \\
Promp Fuse Method            & 77.14          & 74.20           & 74.06           & 73.35         \\
Our Method        &  77.91        & \textbf{75.27}              & \textbf{74.80}                & \textbf{74.36}              \\ 
\bottomrule
\end{tabular}%
\end{table}

\subsection{对比实验设置与结果分析}
% 这里对比实验设置得重新写
如表 \ref{second:mainresult} 所示，我们将本文提出的方法与现有的主流基准模型进行了对比，实验结论分析如下：

(1)相较于基准方法，本章提出的模型(Our Method)在两个数据集上均取得了极具竞争力的表现。特别是在 Twitter-17 数据集上，本章方法在ACC和 F1 值上均大幅超越了所有对比模型。在 Twitter-15 数据集上，尽管 Qwen2-VL-7B 在准确率上略有优势，但本章方法取得了最高的 F1 值。

(2)通过与 Base Model 和 Prompt Fuse Method 的对比，进一步证实了细粒度相关性控制与情感线索引导机制的关键作用。具体来说，Base Model 仅使用二分类相关性来加权图像特征，缺乏明确的情感语义描述引导，导致模型难以充分挖掘图文对中的深层情感交互，限制了性能上限。而Prompt Fuse Method 简单地将图文情感线索拼接在输入端，这种粗粒度的融合方式不仅未能有效利用描述信息，反而可能引入语义噪声，导致模型无法聚焦于关键特征。相比之下，本章方法通过细粒度的相关性控制与显式的情感线索引导，有效解决了上述问题，显著提升了模型对 MASC 任务的适配能力。

(3)虽然 Qwen 系列大模型拥有庞大的参数规模，并在 Twitter-15 的准确率上表现出色，但本章方法在 Twitter-17 的各项指标以及 Twitter-15 的 F1 值上均实现了反超。这表明，在面向 MASC 这类细粒度情感分析任务时，单纯依赖模型参数规模的堆叠并非最优解。通用大模型往往缺乏针对特定图文关系的精细化建模，而本文设计的相关性控制与情感引导机制，能够以更小的参数规模实现更精准的任务特征提取，展现了针对特定任务进行结构化设计的优越性。

\begin{table}[htbp]
  \centering
    \caption{消融实验结果}
    \label{second:Ablation}
\begin{tabular}{lllll}
\toprule
\multirow{2}{*}{Model Name} & \multicolumn{2}{l}{Twitter-15} & \multicolumn{2}{l}{Twitter-17} \\
                            & ACC            & F1             & ACC             & F1            \\
    \hline
Complete Model              & 77.91          & \textbf{75.27}          & 74.79         & \textbf{74.36}       \\
~~~~w/o Relevance Control    & 76.66          & 71.82          & 71.31          & 70.62        \\
~~~~w/o Emotional Clues  & 77.77               & 73.13              & 74.11               & 72.74 \\
~~~~w/o Relevance and  Emotional Clues & 76.60 & 72.70 & 72.96 & 71.37  \\
~~~~w/o Relevance and Text Emotional Clue & 77.91  & 74.50     & 72.77           & 71.36  \\
~~~~w/o Relevance and Image Emotional Clue  & 75.89     & 71.74    & 74.14           & 73.61         \\

\bottomrule
\end{tabular}
\end{table}
\subsection{消融实验分析}

为了验证本章提出的相关性控制模块与图文情感线索的有效性，我们在 Twitter-15 和 Twitter-17 数据集上设计了消融实验：

-w/o Relevance Control：保留图文情感线索，但移除相关性控制模块。此设置用于验证在缺乏相关性约束时，直接引入线索是否有效。

-w/o Emotional Clues：保留相关性控制模块，但移除所有情感线索。此设置用于验证情感线索是否为模型性能提供了重要线索。

-w/o Relevance and  Emotional Clues：同时移除相关性控制和所有情感线索。

-w/o Relevance and Text/Image Emotional Clue：在移除相关性控制的基础上，分别仅保留图像情感线索或仅保留文本情感线索，用于探究不同模态线索对特定数据集的贡献差异。

实验结果如表\ref{second:Ablation}所示，可以得出以下结论：
(1)相较于完整模型(Complete Model)，移除任意模块后的变体在 F1 指标上均出现了不同程度的下滑。这充分证明了细粒度相关性控制与多模态情感线索的结合能够显著增强模型对图文对的情感理解能力，对于提升方面级情感分类任务的判别准确性至关重要。

(2)与移除情感线索引导相比，移除相关性控制模块的实验对模型性能的影响更大，如果仅引入情感线索而缺乏相关性控制，其性能甚至低于没有任何线索的基准模型。表明简单的图文线索堆叠会引入特征冲突或噪声。相关性控制模块在此处起到了关键作用，能够有效过滤无关信息的干扰，解决图文线索冲突问题，确保模型仅利用与目标方面强相关的情感描述。

(3)在分别单独引入图像或文本线索的实验中，我们观察到了一个显著的现象：在 Twitter-15 数据集上，保留图像线索的性能显著优于保留文本线索。这是由于该数据集多包含单一方面目标，图像内容通常直接反映了核心情感，具有较高的信息增益。相反，在 Twitter-17 数据集上，保留文本线索的表现更佳。这是因为 Twitter-17 包含大量多方面的样本，复杂的图像背景往往成为噪声，难以对应到具体的特定方面，此时模型更依赖于文本描述中的精确语义线索。

% 1)相关性机制的作用；情感线索的作用；融合机制的作用；估计还得补一行白板模型
% (1)单独使用文本情感线索描述或图像情感线索描述均能为模型提供有效的情感特征支持。值得注意的是，在 Twitter-15 数据集上，单独利用图像线索的性能甚至优于单独利用文本线索。表明利用MLLM 挖掘出的细粒度模态线索蕴含了丰富的情感语义，是提升分类性能的基础。

% (2)简单的双模态线索融合在 Twitter-15 数据集上的性能要低于单独利用图像线索或文本线索。说明当图像与文本在情感上不一致或存在无关信息时，不加区分的特征融合会引入干扰信号，从而削弱模型的判断能力。

% (3)引入相关性控制机制后，模型在 Twitter-15 数据集上的 F1 值从无控制融合的显著提升。这表明相关性控制机制能够有效缓解模态冲突带来的干扰信号，从而提升模型的情感分类性能。



\begin{table}[htbp]
  \centering
    \caption{相关性质量分析结果}
    \label{second:relevanceQuality}
\begin{tabular}{lllll}
\toprule
\multirow{2}{*}{Model Name} & \multicolumn{2}{l}{Twitter-15} & \multicolumn{2}{l}{Twitter-17} \\
                            & ACC            & F1             & ACC             & F1            \\
    \hline
with Coarse-grained Relevance            & 77.04          & 73.76          & \textbf{74.87}                        & 73.98                        \\
with Fine-grained Relevance            & \textbf{77.91}          & \textbf{75.27}          & 74.80 & \textbf{74.36} \\
~~~~Add 20\% Relevance Noise        & 77.53          & 73.41          & 74.23                        & 72.76                        \\
~~~~Add 50\% Relevance Noise        & 76.47          & 73.22          & 72.77                        & 72.20    \\    
\bottomrule
\end{tabular}
\end{table}

\subsection{相关性质量影响分析}
%
为了深入探究相关性控制模块的质量对模型最终性能的具体影响，我们设计了针对相关性的对比实验。包含以下三种设置：

-粗粒度相关性(Coarse-grained Relevance)：将相关性退化为二值控制，即仅区分图文是否相关，忽略了相关程度的强弱差异。

-细粒度相关性(Fine-grained Relevance)：即本文提出的完整方法，将图文情感相关性分类为情感相关、语义相关和图文无关三种情况。

-相关性噪声干扰(Relevance Noise)：为了验证模型对低质量相关性的敏感度，我们在细粒度相关性的基础上引入人工噪声。具体而言，我们随机选择 20\% 和 50\% 的测试样本，将其的相关性类型替换为随机类型，以此模拟相关性计算失效或对齐错误的情况。

实验结果如表 \ref{second:relevanceQuality} 所示，可以得出以下结论：对比粗粒度与细粒度相关性的实验结果表明，精细化的相关性建模对于提升模型性能至关重要。在F1指标上，细粒度相关性设置在两个数据集中均取得了最优结果。我们认为二元相关性判定不足以捕捉复杂图文对中情感关联，而细粒度相关性能够识别情感相关的图文对保留对情感分类有贡献的关键模态线索。另外通过引入不同比例的噪声干扰，我们观察到模型性能呈现出明显的下降趋势。当引入 20\% 的噪声时，模型在两个数据集上的 F1 值均出现了下滑；而当噪声比例增加至 50\% 时，性能损失进一步扩大。说明了模型对相关性质量的依赖，进一步验证了本文所提出的相关性控制模块在保证图文特征对齐质量方面的核心作用。
\subsection{情感线索质量影响分析}

\section{本章小结}